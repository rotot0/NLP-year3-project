{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network\n",
    "\n",
    "### Resources\n",
    "1. [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "2. [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "3. [Neural machine trranslation by jointly leranring to align and translate](https://arxiv.org/pdf/1409.3215.pdf)\n",
    "4. [Translation with a sequence to sequence network and attention (pytorch)](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "\n",
    "### 1. RNN\n",
    "Recurrent neural networks were based on David Rumelhart's work in 1986. A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.\n",
    "\n",
    "$$y_t, h_t = f(x_t, h_{t-1})$$\n",
    "\n",
    "An Elman network is a three-layer network with the addition of a set of context units. The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step, the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron.\n",
    "\n",
    "$$\n",
    "h_t = \\sigma_{h}(W_h x_t + U_h h_{t-1} + b_h) $$\n",
    "$$y_t = \\sigma_{y}(W_y x_t + U_y h_{t-1} + b_y) $$\n",
    "\n",
    "#### Training\n",
    "Backpropagation through time (BPTT) is a gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. BPTT has difficulty with local optima. With recurrent neural networks, local optima are a much more significant problem than with feed-forward neural networks. The recurrent feedback in such networks tends to create chaotic responses in the error surface which cause local optima to occur frequently, and in poor locations on the error surface.\n",
    "\n",
    "$$\n",
    "y_t, h_t = f(x_{t-1}, h_{t-1}, w) $$\n",
    "$$ y_{t-1}, h_{t-1} = f(x_{t-2}, h_{t-2}, w) $$\n",
    "$$ y_{t-2}, h_{t-2} = f(x_{t-3}, h_{t-3}, w) $$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_t}{\\partial w} = \\frac{\\partial y_t}{\\partial w} + \\frac{\\partial y_t}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w} $$\n",
    "$$\n",
    "\\frac{\\partial h_{t-1}}{\\partial w} = \\frac{\\partial h_{t-1}}{\\partial w} + \\frac{\\partial h_{t-1}}{\\partial h_{t-2}} \\frac{\\partial h_{t-2}}{\\partial w} $$\n",
    "\n",
    "#### Problems\n",
    "1. **[Vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)**. In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range (−1, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the early layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n while the early layers train very slowly.\n",
    "2. **Exploding gradient problem.** When activation functions are used whose derivatives can take on larger values, one risks encountering the related exploding gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.7.0  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('train.txt')\n",
    "text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = text.split('\\n')\n",
    "voc = [v.split(' ') for v in voc]\n",
    "voc.remove([''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = list()\n",
    "for w in voc:\n",
    "    train_pairs.append({'word': w[0], 'sounds': w[1]})\n",
    "    if len(w) > 2:\n",
    "        train_pairs.append({'word': w[0], 'sounds': w[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('pairs.json', 'w') as fp:\n",
    "    fp.write('\\n'.join(json.dumps(i) for i in train_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_inp(w):\n",
    "    return [l for l in w]\n",
    "\n",
    "def tokenize_out(w):\n",
    "    return w.split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "SRC = Field(tokenize=tokenize_inp, init_token = '<sos>', eos_token = '<eos>')\n",
    "TRG = Field(tokenize=tokenize_out, init_token = '<sos>', eos_token = '<eos>', is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TabularDataset(path='pairs.json', format='json', fields={'word': ('src', SRC), 'sounds': ('trg', TRG)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data)\n",
    "TRG.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "train_data, val_data = train_data.split(split_ratio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = BucketIterator(train_data, batch_size=8, device=device)\n",
    "val_iterator = BucketIterator(val_data, batch_size=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import random\n",
    "# from torch import nn\n",
    "# from torch.autograd import Variable\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, input_size, embed_size, hidden_size,\n",
    "#                  n_layers=1, dropout=0.5):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.embed_size = embed_size\n",
    "#         self.embed = nn.Embedding(input_size, embed_size)\n",
    "#         self.gru = nn.GRU(embed_size, hidden_size, n_layers,\n",
    "#                           dropout=dropout, bidirectional=True)\n",
    "\n",
    "#     def forward(self, src, hidden=None):\n",
    "#         embedded = self.embed(src)\n",
    "#         outputs, hidden = self.gru(embedded, hidden)\n",
    "#         # sum bidirectional outputs\n",
    "#         outputs = (outputs[:, :, :self.hidden_size] +\n",
    "#                    outputs[:, :, self.hidden_size:])\n",
    "#         return outputs, hidden\n",
    "\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, hidden_size):\n",
    "#         super(Attention, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "#         self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "#         stdv = 1. / math.sqrt(self.v.size(0))\n",
    "#         self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "#     def forward(self, hidden, encoder_outputs):\n",
    "#         timestep = encoder_outputs.size(0)\n",
    "#         h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "#         encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
    "#         attn_energies = self.score(h, encoder_outputs)\n",
    "#         return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "#     def score(self, hidden, encoder_outputs):\n",
    "#         # [B*T*2H]->[B*T*H]\n",
    "#         energy = F.relu(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
    "#         energy = energy.transpose(1, 2)  # [B*H*T]\n",
    "#         v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [B*1*H]\n",
    "#         energy = torch.bmm(v, energy)  # [B*1*T]\n",
    "#         return energy.squeeze(1)  # [B*T]\n",
    "\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, output_size, embed_size, hidden_size,\n",
    "#                  n_layers=1, dropout=0.2):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.embed_size = embed_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "#         self.n_layers = n_layers\n",
    "\n",
    "#         self.embed = nn.Embedding(output_size, embed_size)\n",
    "#         self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "#         self.attention = Attention(hidden_size)\n",
    "#         self.gru = nn.GRU(hidden_size + embed_size, hidden_size,\n",
    "#                           n_layers, dropout=dropout)\n",
    "#         self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "#     def forward(self, input, last_hidden, encoder_outputs):\n",
    "#         # Get the embedding of the current input word (last output word)\n",
    "#         embedded = self.embed(input).unsqueeze(0)  # (1,B,N)\n",
    "#         embedded = self.dropout(embedded)\n",
    "#         # Calculate attention weights and apply to encoder outputs\n",
    "#         attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n",
    "#         context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n",
    "#         context = context.transpose(0, 1)  # (1,B,N)\n",
    "#         # Combine embedded input word and attended context, run through RNN\n",
    "#         rnn_input = torch.cat([embedded, context], 2)\n",
    "#         output, hidden = self.gru(rnn_input, last_hidden)\n",
    "#         output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
    "#         context = context.squeeze(0)\n",
    "#         output = self.out(torch.cat([output, context], 1))\n",
    "#         output = F.log_softmax(output, dim=1)\n",
    "#         return output, hidden, attn_weights\n",
    "\n",
    "\n",
    "# class Seq2Seq(nn.Module):\n",
    "#     def __init__(self, encoder, decoder, device):\n",
    "#         super(Seq2Seq, self).__init__()\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         self.device = device\n",
    "\n",
    "#     def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "#         batch_size = src.size(1)\n",
    "#         max_len = trg.size(0)\n",
    "#         vocab_size = self.decoder.output_size\n",
    "#         outputs = Variable(torch.zeros(max_len, batch_size, vocab_size)).to(self.device)\n",
    "\n",
    "#         encoder_output, hidden = self.encoder(src)\n",
    "#         hidden = hidden[:self.decoder.n_layers]\n",
    "#         output = Variable(trg.data[0, :])  # sos\n",
    "#         for t in range(1, max_len):\n",
    "#             output, hidden, attn_weights = self.decoder(\n",
    "#                     output, hidden, encoder_output)\n",
    "#             outputs[t] = output\n",
    "#             is_teacher = random.random() < teacher_forcing_ratio\n",
    "#             top1 = output.data.max(1)[1]\n",
    "#             output = Variable(trg.data[t] if is_teacher else top1).cuda()\n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # class Encoder(nn.Module):\n",
    "# #     def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "# #         super().__init__()\n",
    "        \n",
    "# #         self.hid_dim = hid_dim\n",
    "# #         self.n_layers = n_layers\n",
    "        \n",
    "# #         self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "# #         self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "# #         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "# #     def forward(self, src):\n",
    "        \n",
    "# #         #src = [src len, batch size]\n",
    "        \n",
    "# #         embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "# #         #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "# #         outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "# #         #outputs = [src len, batch size, hid dim * n directions]\n",
    "# #         #hidden = [n layers * n directions, batch size, hid dim]\n",
    "# #         #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "# #         #outputs are always from the top hidden layer\n",
    "        \n",
    "# #         return hidden, cell\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.output_dim = output_dim\n",
    "#         self.hid_dim = hid_dim\n",
    "#         self.n_layers = n_layers\n",
    "        \n",
    "#         self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "#         self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "#         self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, input, hidden, cell):\n",
    "        \n",
    "#         #input = [batch size]\n",
    "#         #hidden = [n layers * n directions, batch size, hid dim]\n",
    "#         #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "#         #n directions in the decoder will both always be 1, therefore:\n",
    "#         #hidden = [n layers, batch size, hid dim]\n",
    "#         #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "#         input = input.unsqueeze(0)\n",
    "        \n",
    "#         #input = [1, batch size]\n",
    "        \n",
    "#         embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "#         #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "#         output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "#         #output = [seq len, batch size, hid dim * n directions]\n",
    "#         #hidden = [n layers * n directions, batch size, hid dim]\n",
    "#         #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "#         #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "#         #output = [1, batch size, hid dim]\n",
    "#         #hidden = [n layers, batch size, hid dim]\n",
    "#         #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "#         prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "#         #prediction = [batch size, output dim]\n",
    "        \n",
    "#         return prediction, hidden, cell\n",
    "\n",
    "# # class Seq2Seq(nn.Module):\n",
    "# #     def __init__(self, encoder, decoder, device):\n",
    "# #         super().__init__()\n",
    "        \n",
    "# #         self.encoder = encoder\n",
    "# #         self.decoder = decoder\n",
    "# #         self.device = device\n",
    "        \n",
    "# #         assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "# #             \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "# #         assert encoder.n_layers == decoder.n_layers, \\\n",
    "# #             \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "# #     def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "# #         #src = [src len, batch size]\n",
    "# #         #trg = [trg len, batch size]\n",
    "# #         #teacher_forcing_ratio is probability to use teacher forcing\n",
    "# #         #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "# #         batch_size = trg.shape[1]\n",
    "# #         trg_len = trg.shape[0]\n",
    "# #         trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "# #         #tensor to store decoder outputs\n",
    "# #         outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "# #         #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "# #         hidden, cell = self.encoder(src)\n",
    "        \n",
    "# #         #first input to the decoder is the <sos> tokens\n",
    "# #         input = trg[0,:]\n",
    "        \n",
    "# #         for t in range(1, trg_len):\n",
    "            \n",
    "# #             #insert input token embedding, previous hidden and previous cell states\n",
    "# #             #receive output tensor (predictions) and new hidden and cell states\n",
    "# #             output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "# #             #place predictions in a tensor holding predictions for each token\n",
    "# #             outputs[t] = output\n",
    "            \n",
    "# #             #decide if we are going to use teacher forcing or not\n",
    "# #             teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "# #             #get the highest predicted token from our predictions\n",
    "# #             top1 = output.argmax(1) \n",
    "            \n",
    "# #             #if teacher forcing, use actual next token as next input\n",
    "# #             #if not, use predicted token\n",
    "# #             input = trg[t] if teacher_force else top1\n",
    "        \n",
    "# #         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRnn(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "                \n",
    "        output, hidden = self.rnn(embedded, hidden).to(device)\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 2\n",
    "\n",
    "model = MyRnn(OUTPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIM = len(SRC.vocab)\n",
    "# OUTPUT_DIM = len(TRG.vocab)\n",
    "# ENC_EMB_DIM = 128\n",
    "# DEC_EMB_DIM = 128\n",
    "# HID_DIM = 256\n",
    "# N_LAYERS = 3\n",
    "# ENC_DROPOUT = 0.3\n",
    "# DEC_DROPOUT = 0.3\n",
    "\n",
    "# enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "# dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "# model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyRnn(\n",
       "  (embedding): Embedding(43, 128)\n",
       "  (rnn): LSTM(128, 256, num_layers=2)\n",
       "  (fc_out): Linear(in_features=256, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 938,155 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module,\n",
    "          iterator: torch.utils.data.DataLoader,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for b, batch in enumerate(iterator):\n",
    "        src, trg = batch.src.to(device), batch.trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if b % 500 == 0 and b != 0:\n",
    "            print(\"[%d][loss:%5.2f][pp:%5.2f]\" %\n",
    "                  (b, epoch_loss / (b + 1), math.exp(epoch_loss / (b + 1))))\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_val(model: nn.Module,\n",
    "             iterator: torch.utils.data.DataLoader,\n",
    "             criterion: nn.Module):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src, trg = batch.src.to(device), batch.trg.to(device)\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-a901e736d57d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-130-aa9a2d5e022d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\аким\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-123-3c79f351dfbd>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0membedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\аким\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\аким\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[1;32mc:\\users\\аким\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\аким\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    174\u001b[0m             raise RuntimeError(\n\u001b[0;32m    175\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[1;32m--> 176\u001b[1;33m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             raise RuntimeError(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 4"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "N_EPOCHS = 15\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "best_model  = None\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate_val(model, val_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_model = deepcopy(model)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TabularDataset(path='test_pairs.json', format='json', fields={'word': ('src', SRC), 'sounds': ('trg', TRG)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iterator = BucketIterator(test_data, batch_size=1, device=device, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_to_word(vec):\n",
    "    for el in vec:\n",
    "        print(SRC.vocab.itos[el], end='')\n",
    "    print('\\n')\n",
    "\n",
    "def trg_to_word(vec):\n",
    "    res = ''\n",
    "    for el in vec:\n",
    "        if TRG.vocab.itos[el] == '<eos>':\n",
    "            break\n",
    "        if el > 4:\n",
    "            res += TRG.vocab.itos[el] + '_'\n",
    "    return res[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = torch.transpose(torch.tensor([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]]), 0, 1).to(device)\n",
    "#             src_to_word(src[:,0])\n",
    "#             print(trg_to_word(trg[:,0]))\n",
    "#             print('src:\\n', src.shape)\n",
    "#             print('trg:\\n', trg.shape)\n",
    "            \n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "#             print('predict:\\n', output.argmax(2)[:,1])\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            df.at[i, 'Transcription'] = trg_to_word(output.argmax(2))\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.87385980251314"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_test(best_model.to(device), test_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>P_IH_CH_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>D_IH_S_AA_L_V_ER_Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>S_K_R_AO_N_IY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B_AA_N_N_F_N_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>IH_K_S_IY_D_Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41592</th>\n",
       "      <td>41593</td>\n",
       "      <td>IH_N_AA_K_Y_L_EY_SH_N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41593</th>\n",
       "      <td>41594</td>\n",
       "      <td>N_T_OW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41594</th>\n",
       "      <td>41595</td>\n",
       "      <td>S_K_OW_G_IH_N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41595</th>\n",
       "      <td>41596</td>\n",
       "      <td>HH_EH_SH_N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41596</th>\n",
       "      <td>41597</td>\n",
       "      <td>T_ER_N_AW_F_S_K_IY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41597 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id          Transcription\n",
       "0          1              P_IH_CH_T\n",
       "1          2     D_IH_S_AA_L_V_ER_Z\n",
       "2          3          S_K_R_AO_N_IY\n",
       "3          4         B_AA_N_N_F_N_T\n",
       "4          5          IH_K_S_IY_D_Z\n",
       "...      ...                    ...\n",
       "41592  41593  IH_N_AA_K_Y_L_EY_SH_N\n",
       "41593  41594                 N_T_OW\n",
       "41594  41595          S_K_OW_G_IH_N\n",
       "41595  41596             HH_EH_SH_N\n",
       "41596  41597     T_ER_N_AW_F_S_K_IY\n",
       "\n",
       "[41597 rows x 2 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15482"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['Transcription'] != df2['Transcription']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>D_IH_S_AA_L_V_ER_Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B_AA_N_N_F_N_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>K_P_IH_CH_L_EY_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>M_EY_P_L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>P_ER_S_P_AY_R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>T_IY_N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>M_AA_M_B_AA_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>P_EY_Y_UW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>K_R_IH_S_M_N_Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>S_B_AA_L_OW_Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>OW_V_ER_IY_T_IH_NG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>K_UW_L_M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>R_N_S_IY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>HH_EH_R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>R_IY_T_ER_N_IH_NG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>P_R_EH_V_AY_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>56</td>\n",
       "      <td>M_IH_S_B_HH_EY_V_Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58</td>\n",
       "      <td>IH_L_IH_T_ER_EY_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>S_IH_CH_UW_W_EY_T_IH_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>R_IY_S_IY_P_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>G_AA_D_S_IH_L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>IH_L_IH_K_W_IH_D_IH_T_IY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>S_T_EY_B_L_IH_S_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>L_AA_N_N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>M_AW_N_T_K_EY_T_L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>84</td>\n",
       "      <td>SH_M_NG_K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>85</td>\n",
       "      <td>P_L_AO_R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>D_IH_P_AA_R_T_M_EH_N_L_AY_Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>91</td>\n",
       "      <td>M_K_AW_N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>F_IH_R_EH_N_Z_Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                Transcription\n",
       "1    2           D_IH_S_AA_L_V_ER_Z\n",
       "3    4               B_AA_N_N_F_N_T\n",
       "7    8             K_P_IH_CH_L_EY_T\n",
       "15  16                     M_EY_P_L\n",
       "19  20                P_ER_S_P_AY_R\n",
       "22  23                       T_IY_N\n",
       "28  29                M_AA_M_B_AA_S\n",
       "30  31                    P_EY_Y_UW\n",
       "31  32               K_R_IH_S_M_N_Z\n",
       "36  37                S_B_AA_L_OW_Z\n",
       "38  39           OW_V_ER_IY_T_IH_NG\n",
       "39  40                     K_UW_L_M\n",
       "41  42                     R_N_S_IY\n",
       "42  43                      HH_EH_R\n",
       "44  45            R_IY_T_ER_N_IH_NG\n",
       "51  52                P_R_EH_V_AY_T\n",
       "55  56           M_IH_S_B_HH_EY_V_Z\n",
       "57  58            IH_L_IH_T_ER_EY_T\n",
       "60  61       S_IH_CH_UW_W_EY_T_IH_D\n",
       "61  62                R_IY_S_IY_P_T\n",
       "64  65                G_AA_D_S_IH_L\n",
       "73  74     IH_L_IH_K_W_IH_D_IH_T_IY\n",
       "76  77            S_T_EY_B_L_IH_S_T\n",
       "78  79                     L_AA_N_N\n",
       "82  83            M_AW_N_T_K_EY_T_L\n",
       "83  84                    SH_M_NG_K\n",
       "84  85                     P_L_AO_R\n",
       "87  88  D_IH_P_AA_R_T_M_EH_N_L_AY_Z\n",
       "90  91                     M_K_AW_N\n",
       "96  97              F_IH_R_EH_N_Z_Z"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Transcription'] != df2['Transcription']][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>D_IH_Z_AA_L_V_ER_Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B_OW_N_IH_N_F_N_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>K_AE_P_IH_T_L_EY_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>M_AE_P_L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>P_ER_S_P_AY_ER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>T_AY_N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>M_OW_M_B_AE_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>P_EY_Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>K_R_IH_S_T_M_N_Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>S_B_AE_L_OW_Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>OW_V_ER_IY_EY_T_IH_NG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>K_UW_L_M_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>R_N_K_IY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>EH_R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>R_IH_T_ER_N_IH_NG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>P_R_IY_V_AY_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>56</td>\n",
       "      <td>M_IH_S_B_HH_EY_V_EH_S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58</td>\n",
       "      <td>IH_L_IH_T_ER_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>S_IH_CH_UW_EY_T_IH_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>R_IH_S_IY_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>G_AA_JH_Z_IH_L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>IH_L_IH_K_W_IH_D_T_IY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>S_T_EY_B_L_S_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>L_AA_G_N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>M_AW_N_T_K_AE_S_L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>84</td>\n",
       "      <td>M_M_NG_K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>85</td>\n",
       "      <td>P_L_AO_G_ER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>D_IH_P_AA_R_T_M_EH_N_T_L_AY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>91</td>\n",
       "      <td>M_K_UW_N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>F_AY_R_EH_N_Z_Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                Transcription\n",
       "1    2           D_IH_Z_AA_L_V_ER_Z\n",
       "3    4            B_OW_N_IH_N_F_N_T\n",
       "7    8           K_AE_P_IH_T_L_EY_T\n",
       "15  16                     M_AE_P_L\n",
       "19  20               P_ER_S_P_AY_ER\n",
       "22  23                       T_AY_N\n",
       "28  29                M_OW_M_B_AE_S\n",
       "30  31                       P_EY_Y\n",
       "31  32             K_R_IH_S_T_M_N_Z\n",
       "36  37                S_B_AE_L_OW_Z\n",
       "38  39        OW_V_ER_IY_EY_T_IH_NG\n",
       "39  40                   K_UW_L_M_B\n",
       "41  42                     R_N_K_IY\n",
       "42  43                         EH_R\n",
       "44  45            R_IH_T_ER_N_IH_NG\n",
       "51  52                P_R_IY_V_AY_T\n",
       "55  56        M_IH_S_B_HH_EY_V_EH_S\n",
       "57  58               IH_L_IH_T_ER_T\n",
       "60  61         S_IH_CH_UW_EY_T_IH_D\n",
       "61  62                  R_IH_S_IY_T\n",
       "64  65               G_AA_JH_Z_IH_L\n",
       "73  74        IH_L_IH_K_W_IH_D_T_IY\n",
       "76  77               S_T_EY_B_L_S_T\n",
       "78  79                     L_AA_G_N\n",
       "82  83            M_AW_N_T_K_AE_S_L\n",
       "83  84                     M_M_NG_K\n",
       "84  85                  P_L_AO_G_ER\n",
       "87  88  D_IH_P_AA_R_T_M_EH_N_T_L_AY\n",
       "90  91                     M_K_UW_N\n",
       "96  97              F_AY_R_EH_N_Z_Z"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df['Transcription'] != df2['Transcription']][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submission.csv', columns=['Id', 'Transcription'], index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LSTM (long short-term memory)\n",
    "LSTM was proposed by Sepp Hochreiter and Jürgen Schmidhuber. By introducing Constant Error Carousel (CEC) units, LSTM deals with the vanishing gradient problem. The initial version of LSTM block included cells, input and output gates.\n",
    "\n",
    "In theory, classic RNNs can keep track of arbitrary long-term dependencies in the input sequences. The problem with vanilla RNNs is computational (or practical) in nature: when training a vanilla RNN using back-propagation, the gradients which are back-propagated can \"vanish\" (that is, they can tend to zero) or \"explode\" (that is, they can tend to infinity), because of the computations involved in the process, which use finite-precision numbers. RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow unchanged. However, LSTM networks can still suffer from the exploding gradient problem.\n",
    "\n",
    "The compact forms of the equations for the forward pass of an LSTM unit with a forget gate are below.\n",
    "$$\n",
    "f_t = \\sigma \\big( W_{f} x_t + U_{f} h_{t-1} + b_f \\big) $$\n",
    "$$\n",
    "i_t = \\sigma \\big (W_{i} x_t + U_{i} h_{t-1} + b_i \\big) $$\n",
    "$$\n",
    "o_t = \\sigma(W_{o} x_t + U_{o} h_{t-1} + b_o \\big) $$\n",
    "$$\n",
    "\\tilde{c}_t = \\tanh(W_{c} x_t + U_{c} h_{t-1} + b_c \\big)$$\n",
    "$$\n",
    "c_t = f_t \\ast c_{t-1} + i_t \\ast \\tilde{c}_t $$\n",
    "$$\n",
    "h_t = o_t \\ast \\tanh(c_t)\n",
    "$$\n",
    "\n",
    "### 3. GRU (gated recuent unit)\n",
    "Gated recurrent units are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho. The GRU is like a LSTM with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate. GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs have been shown to exhibit better performance on certain smaller and less frequent datasets.\n",
    "\n",
    "$$\n",
    "z_t = \\sigma \\big( W_z x_t + U h_{t-1} + b_z \\big) $$\n",
    "$$r_t = \\sigma \\big( W_r x_t + U_r h_{t-1} + b_r \\big) $$\n",
    "$$\\hat{h}_t = \\tanh\\big( W_h x_t + U_h(r_t \\ast h_{t-1}) + b_h \\big) $$\n",
    "$$h_t = (1-z_t) \\ast h_{t-1} + z_t \\ast \\hat{h}_t $$\n",
    "\n",
    "\n",
    "**Exercises**\n",
    "1. Try GRU and LSTM model for our sum problem. Which is best?\n",
    "2. Try to train model for 3-digits numbers.\n",
    "3. Add difference (minus) operation.\n",
    "4. Learn this pytorch [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html).\n",
    "5. Take part in the transcription contest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
